// cSpell: ignore Abadi, PACELC, ddia, Kleppmann, Beaugureau, Sebastopol, Linearizability

ifdef::env-github[]
:MERMAID: source, mermaid
endif::[]
ifndef::env-github[]
:MERMAID: mermaid
endif::[]

= Database theory

== CAP theorem

CAP theorem states that any distributed data store can provide at most two (2) of the following qualities.

* *Consistency*
+
A distributed data store has consistency if a read operation returns the same value (reflecting the most recent write) or an error, regardless of the data store instance serving the request.

* *Availability*
+
A distributed data store is available if all non-failing nodes return every request with a response.

* *Partition tolerance*
+
--
A distributed data store has partition tolerance if it can continue operating even in the case of network connectivity disruptions between instances. One of two strategies can be taken:

** Cancel operations during a partition to ensure consistency at the expense of availability.
** Carry out operations during a partition to provide availability at the risk of entering an inconsistent state.
--

The "pick two out of three" presentation can be misleading, because partitions are a type of fault and partitions will happen inevitably <<ddia>>.

=== The PACELC design principle

There is an extension to the CAP theorem called the PACELC design principle which introduces latency as a fourth design characteristic. This results in four different configurations:

[{MERMAID}]
----
flowchart TD
    Partitioned{Partitioned?}
    CvsA{<strong>Tradeoff:</strong><br>Consistency vs.<br>Availability}
    LvsC{<strong>Tradeoff:</strong><br>Latency vs.<br>Consistency}
    PA{PA}
    PC{PC}
    EL{EL}
    EC{EC}

    Partitioned-- (P) yes -->CvsA
    Partitioned-- (E) no -->LvsC

    CvsA-- (A) -->PA
    CvsA-- (C) -->PC

    LvsC-- (L) -->EL
    LvsC-- (C) -->EC
----

[cols="1,2,3"]
|===

|Label
|Priorities
|Implementation

|PA/EL
|prioritize availability and latency over consistency
|Databases built/configured this way often have "eventually consistent" read operations. Requests may not yield the most up-to-data data, because they will answer responses more quickly because there is no need to communicate with other nodes.

|PA/EC
|when there is a partition, choose availability; else, choose consistency
|If possible, try to reach other nodes and reach consensus before responding to requests. If the data store node-to-node requests for consensus time out, respond with potentially out-of-date data.

|PC/EL
|when there is a partition, choose consistency; else, choose latency
|TODO

|PC/EC
|choose consistency at all times
|Because write operations are not considered complete until all nodes (or at least a quorum of nodes) acknowledge replication, there will always be a majority of nodes which agree on current state. If all nodes must acknowledge replication for a write to be committed, then single node can service a read request. If only a majority of nodes need to acknowledge a write in order to be committed, then it is possible that a segmented node will receive a read request, so these quorum-based systems may require inter-node communication in order to ensure consistency.

|===

== Examples

=== Single leader, single follower

[{MERMAID}]
----
block-beta
    columns 3
    regionAUser["Region A user"] space regionAServer["Region A server"]
    space:3
    space:3
    regionBUser["Region B user"] space regionBServer["Region B server"]

    regionAUser-- "write" -->regionAServer
    regionBUser-- "read"  -->regionBServer
    regionAServer-- "Replication fail" -->regionBServer
----

'''

The diagram above is provided to help visualize a scenario concerning a data store with one leader (supports reads and writes) and one follower (supports reads to improve durability, latency, and throughput). The behavior in the presence of a network segmentation will depend on how the data store is built and/or configured.

. If strong consistency is desired, then data will need to be replicated synchronously. This means that the leader will need to wait for acknowledgement that the data was successfully replicated before committing the write.
.. If partitioned, the write will fail, but consistency will be maintained (PC).
.. Else, the write will have increased latency (EC).
. If availability is the top priority, then data should be replicated asynchronously. A write operation can be performed on the leader and committed as soon as it succeeds locally.
.. If partitioned, the write will succeed on the leader despite the replication failure, and the follower will return inconsistent data, improving availability (PA).
.. Else, the system will have reduced write latency by removing the need for the leader to wait for replication success (EL).

[bibliography]
== Sources

* link:https://en.wikipedia.org/wiki/Linearizability[Wikipedia: Linearizability]
* link:https://en.wikipedia.org/wiki/CAP_theorem[Wikipedia: CAP theorem]
* link:https://en.wikipedia.org/wiki/PACELC_design_principle[Wikipedia: PACELC design principle]
* [[[ddia]]] M. Kleppmann, "Consistency and Consensus," in _Designing Data-Intensive Applications_, A. Spencer, Ed., M. Beaugureau, Ed., Sebastopol, CA, USA: O'Reilly, 2017, ch. 8, pp. 336-338.
* [[[problems-with-cap]]] D.J. Abadi, _Problems with CAP, and Yahooâ€™s little known NoSQL system_. Available: https://dbmsmusings.blogspot.com/2010/04/problems-with-cap-and-yahoos-little.html
* [[[consistency-tradeoffs]]] D.J. Abadi, _Consistency Tradeoffs in Modern Distributed Database System Design_. Available: https://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf
